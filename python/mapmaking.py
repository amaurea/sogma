import numpy as np, os
import time
from pixell import utils, bunch, enmap, colors, wcsutils, bench
from . import nmat, pmat, tiling, gutils, device
from .logging import L

class MLMapmaker:
	def __init__(self, signals=[], dev=None, noise_model=None, dtype=np.float32, verbose=False):
		"""Initialize a Maximum Likelihood Mapmaker.
		Arguments:
		* signals: List of Signal-objects representing the models that will be solved
		  jointly for. Typically this would be the sky map and the cut samples. NB!
		  The way the cuts currently work, they *MUST* be the first signal specified.
		  If not, the equation system will be inconsistent and won't converge.
		* noise_model: A noise model constructor which will be used to initialize the
		  noise model for each observation. Can be overriden in add_obs.
		* dtype: The data type to use for the time-ordered data. Only tested with float32
		* verbose: Whether to print progress messages. Not implemented"""
		self.signals  = signals
		self.dev      = dev or device.get_device()
		self.dtype    = dtype
		self.verbose  = verbose
		self.noise_model = noise_model or nmat.NmatDetvecs(dev=dev)
		self.reset()
	def reset(self):
		"""Reset the mapmaker, as if it had just been constructed. Also
		resets its signals, so it's ready to make new maps."""
		self.data     = []
		self.dof      = MultiZipper()
		self.ready    = False
		for signal in self.signals:
			signal.reset()
	def add_obs(self, id, obs, deslope=True, noise_model=None):
		# Prepare our tod
		t1 = time.time()
		ctime  = obs.ctime
		srate  = (len(ctime)-1)/(ctime[-1]-ctime[0])
		tod    = obs.tod.astype(self.dtype, copy=False)
		t2 = time.time()
		if deslope:
			utils.deslope(tod, w=5, inplace=True)
		t3 = time.time()
		gtod = self.dev.pools["tod"].array(tod)
		del tod
		# Allow the user to override the noise model on a per-obs level
		if noise_model is None: noise_model = self.noise_model
		# Build the noise model from the obs unless a fully
		# initialized noise model was passed
		if noise_model.ready:
			iN = noise_model
		else:
			try:
				iN = noise_model.build(gtod, srate=srate)
			except Exception as e:
				msg = f"FAILED to build a noise model for observation='{id}' : '{e}'"
				raise gutils.RecoverableError(msg)
		t4 = time.time()
		# And apply it to the tod
		gtod = iN.apply(gtod)
		t5 = time.time()
		# This is our last chance to safely abort, so check that our data makes sense
		if not self.dev.np.isfinite(self.dev.np.sum(gtod)):
			raise gutils.RecoverableError(f"Invalid value in N\"tod")
		# Add the observation to each of our signals
		for signal in self.signals:
			signal.add_obs(id, obs, iN, gtod)
		# TODO: update our stats/info here
		t6 = time.time()
		L.print("Init sys trun %6.3f ds %6.3f Nb %6.3f N %6.3f add sigs %6.3f %s" % (t2-t1, t3-t2, t4-t3, t5-t4, t6-t5, id), level=2)
		# Save only what we need about this observation
		self.data.append(bunch.Bunch(id=id, ndet=len(obs.dets), nsamp=len(ctime),
			dets=obs.dets, iN=iN))
	def prepare(self):
		if self.ready: return
		t1 = time.time()
		for signal in self.signals:
			signal.prepare()
			self.dof.add(signal.dof)
		t2 = time.time()
		L.print("Prep sys %6.3f" % (t2-t1), level=2)
		self.ready = True
	def A(self, x):
		t1 = time.time()
		iwork = [signal.to_work(m) for signal,m in zip(self.signals,self.dof.unzip(x))]
		owork = [signal.owork()    for signal in self.signals]
		t2 = time.time()
		for di, data in enumerate(self.data):
			# This is the main place that needs to change for the GPU implementation
			ta1  = self.dev.time()
			gtod = self.dev.pools["tod"].empty([data.ndet, data.nsamp], self.dtype)
			ta2  = self.dev.time()
			for si, signal in reversed(list(enumerate(self.signals))):
				signal.precalc_setup(data.id)
				signal.forward(data.id, gtod, iwork[si])
			ta3 = self.dev.time()
			data.iN.apply(gtod)
			ta4 = self.dev.time()
			for si, signal in enumerate(self.signals):
				signal.backward(data.id, gtod, owork[si])
				signal.precalc_free(data.id)
			ta5 = self.dev.time()
			self.dev.garbage_collect()
			ta6 = self.dev.time()
			L.print("A z %6.3f P %6.3f N %6.3f P' %6.3f gc %6.4f %s %4d %6d" % (ta2-ta1, ta3-ta2, ta4-ta3, ta5-ta4, ta6-ta5, data.id, *gtod.shape), level=2)
		t3 = self.dev.time()
		result = self.dof.zip([signal.from_work(w) for signal,w in zip(self.signals,owork)])
		t4 = self.dev.time()
		L.print("A prep %6.3f PNP %6.3f finish %6.3f" % (t2-t1, t3-t2, t4-t3), level=2)
		return result
	def M(self, x):
		t1 = self.dev.time()
		iwork = self.dof.unzip(x)
		result = self.dof.zip([signal.precon(w) for signal, w in zip(self.signals, iwork)])
		t2 = self.dev.time()
		L.print("M %6.3f" % (t2-t1), level=2)
		return result
	def solve(self, maxiter=500, maxerr=1e-6):
		self.prepare()
		rhs    = self.dof.zip([signal.rhs for signal in self.signals])
		solver = utils.CG(self.A, rhs, M=self.M, dot=self.dof.dot)
		#solver = utils.Minres(self.A, rhs, dot=self.dof.dot)
		while solver.i < maxiter and solver.err > maxerr:
			t1 = time.time()
			solver.step()
			x  = self.dof.unzip(solver.x)
			t2 = time.time()
			yield bunch.Bunch(i=solver.i, err=solver.err, x=x, t=t2-t1)

class Signal:
	"""This class represents a thing we want to solve for, e.g. the sky, ground, cut samples, etc."""
	def __init__(self, name, ofmt, outputs, ext):
		"""Initialize a Signal. It probably doesn't make sense to construct a generic signal
		directly, though. Use one of the subclasses.
		Arguments:
		* name: The name of this signal, e.g. "sky", "cut", etc.
		* ofmt: The format used when constructing output file prefix
		* outputs: Which of our information to output. Empty list or
		  None to output nothing. "all" for all valid products.
		* ext: The extension used for the files.
		"""
		self.name   = name
		self.ofmt   = ofmt
		self.ext    = ext
		self.dof    = None
		self.ready  = False
		self.set_outputs(outputs)
	def reset(self):
		self.dof   = None
		self.ready = False
	def add_obs(self, id, obs, iN, iNd): pass
	def prepare(self): self.ready = True
	def forward (self, id, tod, x): pass
	def backward(self, id, tod, x): pass
	def precalc_setup(self, id): pass
	def precalc_free (self, id): pass
	def precon(self, x): return x
	def to_work  (self, x): return x.copy()
	def from_work(self, x): return x
	def owork(self): raise NotImplementedError
	def set_outputs(self, outputs):
		self.outputs= check_outputs(outputs, self.valid_outputs, self.__class__.__name__)
	def written(self, prefix): raise NotImplementedError
	def write   (self, prefix, x, tag="dummy", suffix="", force=False): pass
	def write_misc(self, prefix): pass
	valid_outputs = []

class SignalMap(Signal):
	"""Signal describing a sky map."""
	def __init__(self, shape, wcs, comm, dev=None, name="sky", ofmt="{name}",
			outputs=["map","ivar"], ext="fits", dtype=np.float32, ibuf=None, obuf=None,
			recenter=None, sys=None, interpol=None, autocrop=False):
		"""Signal describing a sky map in the coordinate system given by "sys", which defaults
		to equatorial coordinates."""
		Signal.__init__(self, name, ofmt, outputs, ext)
		self.sys   = sys or "cel"
		if self.sys not in ["cel","equ","hor"]:
			raise NotImplementedError("Coordinate system rotation not implemented yet")
		self.recenter = recenter
		self.dtype = dtype
		self.interpol = interpol
		self.data  = {}
		self.comps = "TQU"
		self.ncomp = 3
		self.comm  = comm
		self.autocrop = autocrop
		self.dev   = dev or device.get_device()
		# Set up our internal tiling
		self.shape,  self.wcs = shape, wcs
		self.fshape, self.fwcs, self.pixbox = tiling.infer_fullsky_geometry(shape, wcs)
		if autocrop: self.pixbox = "auto"
		# Buffers to use
		self.ibuf = self.dev.pools[name+"_iwork"]
		self.obuf = self.dev.pools[name+"_owork"]
		# Not sure how to avoid code duplication with reset here,
		# in light of the inheritance
		self.ids      = []
		self.tiledist = None
		# Dynamic RHS map which we will accumulate into
		self.drhs  = self.dev.lib.DynamicMap(*self.fshape, self.dtype)
		# Used as reference point for time maps
		self.tref  = np.inf
	def reset(self):
		Signal.reset(self)
		self.ids      = []
		self.tiledist = None
		self.drhs  = self.dev.lib.DynamicMap(*self.fshape, self.dtype)
	def add_obs(self, id, obs, iN, iNd, pmap=None):
		"""Add and process an observation, building the pointing matrix
		and our part of the RHS. "obs" should be an Observation axis manager,
		iN a noise model, representing the inverse noise covariance matrix,
		and iNd the result of applying the noise model to the detector time-ordered data.
		"""
		ctime     = obs.ctime
		self.tref = min(self.tref, ctime[0])
		t1        = time.time()
		# could pass this in, but fast to construct. Should ideally match what's used in
		# signal_cut to be safest, but only uses .clear which should be the same for all
		# of them anyway
		if pmap is None:
			pmap = pmat.PmatMap(self.fshape, self.fwcs, obs.ctime, obs.boresight, obs.point_offset, obs.polangle, sys=self.sys, response=obs.response, recenter=self.recenter, dev=self.dev, dtype=iNd.dtype)
			self.dev.garbage_collect()
		# Precompute pointing for the upcoming pmap observations
		# Accumulate local rhs
		t2 = time.time()
		pmap.backward(iNd, self.drhs)
		self.dev.garbage_collect()
		t3 = time.time()
		L.print("Init map pmat %6.3f rhs %6.3f %s" % (t2-t1,t3-t2,id), level=2)
		# Save the per-obs things we need. Just the pointing matrix in our case.
		# Nmat and other non-Signal-specific things are handled in the mapmaker itself.
		self.data[id] = bunch.Bunch(pmap=pmap, iN=iN, tod_shape=iNd.shape, tod_dtype=iNd.dtype, ctime=ctime)
		self.ids.append(id)
	def calc_hits(self, weight=False, fill=1, name="hits"):
		"""Calculate the local div or hits map. Tiling must be
		finalized before we do this. Will need a different function for non-scalar div"""
		# Could save one map buffer by calculating these separately,
		# but that would be slower
		glhits = self.tiledist.gwmap(buf=self.obuf, dtype=self.dtype) # LocalMap
		for i, id in enumerate(self.ids):
			d   = self.data[id]
			t1  = time.time()
			tod    = self.dev.pools["tod"].empty(d.tod_shape, d.tod_dtype)
			if isinstance(fill, str):
				if fill == "time": tod[:] = self.dev.np.array(d.ctime)-self.tref
				else: raise ValueError("Unrecognized fill '%s'" % fill)
			else: tod[:] = fill
			if weight: d.iN.white(tod)
			# Shouldn't there be a pcut.clear here?
			d.pmap.backward(tod, glhits)
			t2  = time.time()
			L.print("Init map %s %d %6.3f %s" % (name, weight, t2-t1,id), level=2)
		return glhits
	def prepare(self):
		"""Called when we're done adding everything. Sets up the map distribution,
		degrees of freedom and preconditioner."""
		if self.ready: return
		mybench = bench.Bench()
		with mybench.mark("fin"):
			self.tref = min(self.comm.allgather(self.tref))
			if not np.isfinite(self.tref): self.tref = 0
			# Ok, this is where we finally know enough to finish the tiledist
			glrhs  = self.drhs.finalize(); del self.drhs
			self.tiledist = tiling.TileDistribution(self.fshape, self.fwcs,
					glrhs.pixelization, self.comm, pixbox=self.pixbox, dev=self.dev)
		# Reduce to global tiles and free up the arrays on the dpu
		with mybench.mark("red"):
			self.rhs   = self.tiledist.gwmap2dmap(glrhs);  del glrhs
		with mybench.mark("misc"):
			if "hits" in self.outputs:
				glhits = self.calc_hits(weight=False)
				self.dev.garbage_collect()
		with mybench.mark("red"):
			if "hits" in self.outputs:
				self.hits  = self.tiledist.gwmap2dmap(glhits); del glhits
				self.hits  = self.hits[:,:1]
		with mybench.mark("div"):
			gldiv = self.calc_hits(weight=True, name="div")
			self.dev.garbage_collect()
		with mybench.mark("red"):
			self.div   = self.tiledist.gwmap2dmap(gldiv); del gldiv
			self.div   = self.div[:,:1]
		# Set up our degrees of freedom
		self.dof   = ArrayZipper(self.rhs.shape, dtype=self.dtype, comm=self.comm)
		with mybench.mark("prec"):
			self.idiv  = gutils.safe_invert_ivar(self.div)
		with mybench.mark("misc"):
			if "time" in self.outputs:
				gltime     = self.calc_hits(weight=True, fill="time", name="time")
		with mybench.mark("red"):
			if "time" in self.outputs:
				self.tmap  = self.tiledist.gwmap2dmap(gltime)[:,:1]; del gltime
		with mybench.mark("misc"):
			if "time" in self.outputs:
				self.tmap *= self.idiv
		L.print("Prep fin %6.3f div %6.3f misc %6.3f red %6.3f prec %6.3f" % tuple([
			mybench.t_tot[name] for name in ["fin","div","misc","red","prec"]]), level=2)
		self.ready = True
	def forward(self, id, gtod, glmap, tmul=1):
		"""map2tod operation. For tiled maps, the map should be in work distribution,
		as returned by unzip. Adds into tod."""
		if id not in self.data: return # Should this really skip silently like this?
		if tmul != 1: gtod *= tmul
		self.data[id].pmap.forward(gtod, glmap)
	def backward(self, id, gtod, glmap, mmul=1):
		"""tod2map operation. For tiled maps, the map should be in work distribution,
		as returned by unzip. Adds into map"""
		if id not in self.data: return
		if mmul != 1: glmap *= mmul
		self.data[id].pmap.backward(gtod, glmap)
	def precalc_setup(self, id, reset_buffer=True): self.data[id].pmap.precalc_setup(reset_buffer=reset_buffer)
	def precalc_free (self, id): self.data[id].pmap.precalc_free()
	def precon(self, gmap):
		return self.idiv * gmap
	def to_work(self, gmap): return self.tiledist.dmap2gwmap(gmap, buf=self.ibuf)
	def from_work(self, glmap): return self.tiledist.gwmap2dmap(glmap)
	def owork(self): return self.tiledist.gwmap(self.obuf, dtype=self.dtype)
	def written(self, prefix):
		done  = True
		for tag in self.outputs:
			oname = self.ofmt.format(name=self.name)
			oname = "%s%s_%s.%s" % (prefix, oname, tag, self.ext)
			done  = done and os.path.exists(oname)
		return done
	def write(self, prefix, m, tag="map", suffix="", force=False, extra={}):
		if not force and tag not in self.outputs: return
		oname = self.ofmt.format(name=self.name)
		oname = "%s%s_%s%s.%s" % (prefix, oname, tag, suffix, self.ext)
		omap  = self.tiledist.dmap2omap(m)
		if self.comm.rank == 0:
			enmap.write_map(oname, omap, extra=extra)
		return oname
	def write_misc(self, prefix):
		# Need the if test here despite write also testing, because
		# we're referring to members that might not exist
		if "rhs"  in self.outputs: self.write(prefix, self.rhs,           tag="rhs")
		if "ivar" in self.outputs: self.write(prefix, self.div[:,0],      tag="ivar")
		if "hits" in self.outputs: self.write(prefix, self.hits[:,0],     tag="hits")
		if "bin"  in self.outputs: self.write(prefix, self.idiv*self.rhs, tag="bin")
		if "time" in self.outputs:
			self.write(prefix, self.tmap[:,0], tag="time", extra={"TREF":self.tref})
	valid_outputs = ["map","ivar","hits","bin","rhs","time"]

# What do we need for multi-band mapmaking?
# Just loop over SignalMaps, but override their TileDistribution
# ompi to a new one built from their pixbox union

class SignalMapMulti(Signal):
	"""Signal describing a multiband sky map."""
	def __init__(self, shape, wcs, bands, comm, dev=None, name="sky", ofmt="{name}", outputs=["map","ivar"],
			ext="fits", dtype=np.float32, ibuf=None, obuf=None, recenter=None,
			sys=None, interpol=None, autocrop=False):
		self.mapsigs = [
			SignalMap(shape, wcs, comm, dev=dev, name=band + "_" + name, ofmt=ofmt, outputs=outputs,
				ext=ext, dtype=dtype, ibuf=ibuf, obuf=obuf, recenter=recenter,
				sys=sys, interpol=interpol, autocrop=autocrop)
			for band in bands]
		# Must happen after self.mapsigs has been created
		Signal.__init__(self, name, ofmt, outputs, ext)
		self.bands = bands
		self.dev   = dev or device.get_device()
		self.data  = {}
		self.ids   = []
		self.ready = False
	def reset(self):
		for mapsig in self.mapsigs: mapsig.reset()
	def add_obs(self, id, obs, iN, iNd):
		# Find the det-ranges for each of our bands in obs
		band_info = []
		for bi, band in enumerate(self.bands):
			inds   = np.where(obs.bands == band)[0]
			# Skip band that's not present
			if len(inds) == 0: continue
			d1, d2 = inds[0], inds[-1]+1
			# Ensure that they're contiguous!
			assert d2-d1 == len(inds), "Detectors must be sorted by band!"
			# Make obs for the band subset
			bobs = obs.copy()
			bobs.point_offset = obs.point_offset[d1:d2]
			bobs.polangle     = obs.polangle[d1:d2]
			bobs.response     = obs.response[:,d1:d2] if obs.response is not None else None
			# Also need to restrict iN. iN is only used for the precondtioner, so
			# only need to handle this part. This is a bit hacky though, as theoretically
			# a future preconditioner could use more of iN...
			biN = nmat.NmatWhite(ivar=iN.ivar[d1:d2], nwin=iN.nwin, dev=self.dev)
			# And finally forward to our band signals
			self.mapsigs[bi].add_obs(id, bobs, biN, iNd[d1:d2])
			band_info.append((bi,band,d1,d2))
		self.data[id] = bunch.Bunch(band_info=band_info)
		self.ids.append(id)
	def prepare(self):
		"""Called when we're done adding everything. Sets up the map distribution,
		degrees of freedom and preconditioner."""
		# Make all our children prepare, and build up the total degrees of freedom
		self.dof = MultiZipper()
		for mapsig in self.mapsigs:
			mapsig.prepare()
			self.dof.add(mapsig.dof)
		self.rhs = [mapsig.rhs for mapsig in self.mapsigs]
		## Harmonize the output pixelization, so all bands cover the same pixels.
		## This makes plotting easier, but also means we can't have different
		## resolution for the different band maps. But to allow that, we would need
		## to pass in multiple geometries anyway, which we don't do. So this doesn't
		## make us less general than we already area.
		## This is a bit hacky, since it messes with the internals of TileDistribution.
		#pixboxes = np.array([mapsig.tiledist.pixbox for mapsig in self.mapsigs])
		#pixbox   = np.array([np.min(pixboxes[:,0,:],0),np.max(pixboxes[:,1,:],0)])
		#for mi, mapsig in enumerate(self.mapsigs):
		#	td = mapsig.tiledist
		#	mapsig.pixbox = pixbox
		#	_, mapsig.tiledist.pwcs = enmap.crop_geometry(mapsig.shape, mapsig.wcs, pixbox=pixbox, recenter=True)
		#	td.ompi = tiling.build_omap_mpi(mapsig.shape, mapsig.wcs, td.tshape, td.dist.cell_inds, mapsig.comm, pixbox=pixbox)
		self.ready = True
	def forward(self, id, gtod, glmap, tmul=1):
		"""map2tod operation. For tiled maps, the map should be in work distribution,
		as returned by unzip. Adds into tod."""
		if id not in self.data: return # Should this really skip silently like this?
		for bi, band, d1, d2 in self.data[id].band_info:
			self.mapsigs[bi].forward(id, gtod[d1:d2], glmap[bi], tmul=tmul)
	def backward(self, id, gtod, glmap, mmul=1):
		"""tod2map operation. For tiled maps, the map should be in work distribution,
		as returned by unzip. Adds into map"""
		if id not in self.data: return
		for bi, band, d1, d2 in self.data[id].band_info:
			self.mapsigs[bi].backward(id, gtod[d1:d2], glmap[bi], mmul=mmul)
	def precalc_setup(self, id):
		self.dev.pools["pointing"].reset()
		self.dev.pools["plan"].reset()
		for bi, band, d1, d2 in self.data[id].band_info:
			self.mapsigs[bi].precalc_setup(id, reset_buffer=False)
	def precalc_free (self, id):
		for bi, band, d1, d2 in self.data[id].band_info:
			self.mapsigs[bi].precalc_free(id)
	def precon(self, gmap):
		return [mapsig.precon(m) for mapsig,m in zip(self.mapsigs, gmap)]
	def to_work(self, gmap):
		return [mapsig.to_work(m) for mapsig, m in zip(self.mapsigs, gmap)]
	def from_work(self, glmap):
		return [mapsig.from_work(m) for mapsig, m in zip(self.mapsigs, glmap)]
	def owork(self):
		return [mapsig.owork() for mapsig in self.mapsigs]
	def set_outputs(self, outputs):
		for mapsig in self.mapsigs:
			mapsig.set_outputs(outputs)
	def written(self, prefix):
		for mapsig in self.mapsigs:
			if not mapsig.written(prefix):
				return False
		return True
	def write(self, prefix, m, tag="map", suffix="", force=False):
		onames = []
		for bi, band in enumerate(self.bands):
			oname = self.mapsigs[bi].write(prefix, m[bi], tag=tag, suffix=suffix, force=force)
			onames.append(oname)
		return ", ".join(onames)
	def write_misc(self, prefix):
		for mapsig in self.mapsigs:
			mapsig.write_misc(prefix)
	valid_outputs = SignalMap.valid_outputs

class SignalCutFull(Signal):
	# Placeholder for when we have a gpu implementation
	def __init__(self, comm, dev=None, name="cut", ofmt="{name}_{rank:02}", dtype=np.float32,
			outputs=[]):
		"""Signal for handling the ML solution for the values of the cut samples."""
		Signal.__init__(self, name, ofmt, outputs, ext="hdf")
		self.comm  = comm
		self.dev   = dev or device.get_device()
		self.dtype = dtype
		self.off   = 0
		self.rhs   = []
		self.idiv  = []
		self.data  = {}
	def reset(self):
		Signal.reset(self)
		self.off   = 0
		self.rhs   = []
		self.idiv  = []
		self.data  = {}
	def add_obs(self, id, obs, iN, iNd):
		"""Add and process an observation. "obs" should be an Observation axis manager,
		iN a noise model, representing the inverse noise covariance matrix,
		and iNd the result of applying the noise model to the detector time-ordered data."""
		iNd     = iNd.copy() # This copy can be avoided if build_obs is split into two parts
		pcut    = pmat.PmatCutFull(obs.cuts, dev=self.dev)
		# Build our RHS
		obs_rhs = self.dev.np.zeros(pcut.ndof, self.dtype)
		pcut.backward(iNd, obs_rhs)
		obs_rhs = obs_rhs.get()
		# Build our preconditioner.
		obs_div = self.dev.np.ones(pcut.ndof, self.dtype)
		iNd[:]   = 0
		pcut.forward(iNd, obs_div)
		iN.white(iNd)
		pcut.backward(iNd, obs_div)
		obs_idiv = 1/self.dev.get(obs_div) # back to the cpu
		self.data[id] = bunch.Bunch(pcut=pcut, i1=self.off, i2=self.off+pcut.ndof)
		self.off += pcut.ndof
		self.rhs.append(obs_rhs)
		self.idiv.append(obs_idiv)
		self.dev.garbage_collect()
	def prepare(self):
		"""Process the added observations, determining our degrees of freedom etc.
		Should be done before calling forward and backward."""
		if self.ready: return
		self.rhs = np.concatenate(self.rhs)  if len(self.rhs) > 0 else np.zeros(0, self.dtype)
		self.idiv= np.concatenate(self.idiv) if len(self.rhs) > 0 else np.zeros(0, self.dtype)
		self.dof = ArrayZipper(self.rhs.shape, dtype=self.dtype, comm=self.comm)
		self.ready = True
	def forward(self, id, gtod, gjunk):
		if id not in self.data: return
		d = self.data[id]
		d.pcut.forward(gtod, gjunk[d.i1:d.i2])
	def precon(self, junk):
		return junk*self.idiv
	def backward(self, id, gtod, gjunk):
		# This function doesn't just accumulate into junk, it also
		# zeros out the cut samples from tod. This lets all the other signals
		# not have to deal with the cuts, as long as the cuts are the first
		# in the signal list (so first backwards, last forwards)
		if id not in self.data: return
		d = self.data[id]
		d.pcut.backward(gtod, gjunk[d.i1:d.i2])
	# TODO: Check if these should use mempools
	def to_work  (self, x): return self.dev.np.array(x)
	def from_work(self, x): return self.dev.get(x)
	def owork(self): return self.dev.np.zeros(self.rhs.shape, self.rhs.dtype)
	def written(self, prefix):
		if self.comm is None:
			rank = 0
		else:
			rank = self.comm.rank
		done = True
		for tag in self.outputs:
			oname = self.ofmt.format(name=self.name, rank=rank)
			oname = "%s%s_%s.%s" % (prefix, oname, tag, self.ext)
			done  = done and os.path.exists(oname)
		if self.comm is not None:
			done = self.comm.allreduce(done)==self.comm.size
		return done
	def write(self, prefix, m, tag="map", suffix="", force=False):
		if not force and tag not in self.outputs: return
		import h5py
		if self.comm is None:
			rank = 0
		else:
			rank = self.comm.rank
		oname = self.ofmt.format(name=self.name, rank=rank)
		oname = "%s%s_%s%s.%s" % (prefix, oname, tag, suffix, self.ext)
		with h5py.File(oname, "w") as hfile:
			hfile["data"] = m
		return oname
	valid_outputs = ["map"]

class SignalCutPoly(SignalCutFull):
	def __init__(self, comm, dev=None, order=3, bsize=400, precon="none", name="cut",
			ofmt="{name}_{rank:02}", dtype=np.float32, outputs=[]):
		"""Signal for handling the ML solution for the values of the cut samples."""
		SignalCutFull.__init__(self, comm, dev=dev, name=name, ofmt=ofmt, dtype=dtype, outputs=outputs)
		self.order  = order
		self.bsize  = bsize
		self.basis  = gutils.legbasis(order, bsize)
		self.prec   = precon
		if precon != "none":
			raise NotImplementedError("SignalCutPoly precons other than 'none' currently don't work")
		if precon == "leginv":
			self.ibases = gutils.leginverses(self.basis)
	def add_obs(self, id, obs, iN, iNd):
		"""Add and process an observation. "obs" should be an Observation axis manager,
		iN a noise model, representing the inverse noise covariance matrix,
		and iNd the result of applying the noise model to the detector time-ordered data."""
		iNd     = iNd.copy() # This copy can be avoided if build_obs is split into two parts
		pcut    = pmat.PmatCutPoly(obs.cuts, basis=self.basis, dev=self.dev)
		# Build our RHS
		obs_rhs = self.dev.np.zeros(pcut.ndof, self.dtype)
		pcut.backward(iNd, obs_rhs)
		obs_rhs = self.dev.get(obs_rhs)
		# Build our preconditioner. We have precomputed the basis.dot(basis.T) inverse for
		# all block lengths, so we just read out the appropriate one for each block.
		# TODO: Understand why the none-preconditioner suddenly works fine, while
		# the other two breaks convergence. Apparently this only happens when I include
		# 1/ivar in them, otherwise they don't hurt but also don't beat none. Very
		# strange.
		if self.prec == "leginv":
			obs_idiv = self.ibases[self.dev.get(pcut.lens)-1] / self.dev.get(iN.ivar[pcut.dets])[:,None,None]
		elif self.prec == "var":
			obs_idiv = 1/self.dev.get(iN.ivar[pcut.dets])
		elif self.prec == "none":
			obs_idiv = [1]
		else: raise ValueError("Unknown precon '%s'" % str(self.prec))
		self.data[id] = bunch.Bunch(pcut=pcut, i1=self.off, i2=self.off+pcut.ndof)
		self.off += pcut.ndof
		self.rhs.append(obs_rhs)
		self.idiv.append(obs_idiv)
		self.dev.garbage_collect()
	def precon(self, junk):
		# For some reason this fails with negative residuals when I use this well-motivated
		# and symmetric preconditioner, but suddenly works fine when I use no preconditioner at all.
		if self.prec == "leginv":
			bjunk = junk.reshape(-1, self.basis.shape[0])
			bjunk = np.einsum("iab,ib->ia", self.idiv, bjunk)
			return bjunk.reshape(-1)
		elif self.prec == "var":
			bjunk = junk.reshape(-1, self.basis.shape[0]).copy()
			bjunk *= self.idiv[:,None]
			return bjunk.reshape(-1)
		elif self.prec == "none":
			return junk.copy()
		else: raise ValueError("Unknown precon '%s'" % str(self.prec))

# Actually, doing this as an actual Signal might be cleanest.
# Requries no changes to Mapmaker, make_map, etc.
# Slightly hacky since it would have no degrees of freedom though.
class SignalInfo(Signal):
	def __init__(self, comm, dev=None):
		Signal.__init__(self, name="info", ofmt="{name}", outputs=["info"], ext="hdf")
		self.comm  = comm
		self.dev   = dev if dev is not None else device.get_gevice()
		self.dof   = ArrayZipper(0, dtype=np.float32)
		self.rhs   = np.zeros(0, dtype=np.float32)
		self.info  = []
	def reset(self):
		self.info  = []
		self.ready = False
	def add_obs(self, id, obs, iN, iNd):
		srate = (len(obs.ctime)-1)/(obs.ctime[-1]-obs.ctime[0])
		# Get the scanning info. These are *after* the pointing
		# model has been applied
		def leftw(arr):
			v1, v2 = utils.minmax(arr)
			return v1, v2-v1
		def cenw(arr):
			v1, v2 = utils.minmax(arr)
			return (v1+v2)/2, v2-v1
		ctime,  dur   = leftw(obs.ctime)
		az,     waz   = cenw(obs.boresight[1])
		el,     wel   = cenw(obs.boresight[0])
		roll,   wroll = cenw(obs.boresight[2])
		# Detector info
		dsens = (iN.ivar*srate)**-0.5
		# Center and radius per band
		self.info.append(bunch.Bunch(
			id=id, ctime=ctime, dur=dur, az=az, waz=waz, el=el, wel=wel, roll=roll, wroll=wroll,
			dsens=dsens, dets=obs.dets, detids=obs.detids, bands=obs.bands,
			offs=obs.point_offset))
	def owork(self): return np.zeros(self.rhs.shape, self.rhs.dtype)
	def prepare(self):
		# high-level gather. Hopefully not too high overhead
		info = self.comm.gather(self.info)
		if self.comm.rank == 0: self.info = sum(info,[])
		self.ready = True
	def written(self, prefix):
		return os.path.isfile(prefix + "info.hdf")
	def write_misc(self, prefix):
		# We only have a misc product, nothing we're actually solving for
		if "info" not in self.outputs: return
		if self.comm.rank != 0: return
		nobs   = len(self.info)
		# Loop through and find the full set of ids and dets
		ids, detids = [], []
		bands = {}
		for row in self.info:
			ids.append(row.id)
			detids.append(row.detids)
			for detid, band in zip(row.detids, row.bands):
				bands[detid] = band
		ids    = np.char.encode(np.array(ids))
		detids = np.unique(np.concatenate(detids))
		bands  = np.array([bands[detid] for detid in detids])
		ndet   = len(detids)
		# Sort dets by detid and obs by id
		order_obs = np.argsort(ids)
		order_det = np.argsort(detids)
		detids    = detids[order_det]
		bands     = bands [order_det]
		# Get the sensitivity per band
		ubands, order, edges = utils.find_equal_groups_fast(bands)
		nband     = len(ubands)
		# Some of our info fits nicely into a simple per-obs table
		obstab = np.zeros(nobs, [("id",ids.dtype),("ctime","f"),("dur","f"),
			("az","f"),("waz","f"),("el","f"),("wel","f"),
			("roll","f"),("wroll","f"),("ndet","i"),("sens","%df"%nband)]).view(np.recarray)
		simple_fields = ["ctime","dur","az","waz","el","wel","roll","wroll"]
		deg_fields = ["az","waz","el","wel","roll","wroll"]
		for i, ri in enumerate(order_obs):
			row = self.info[ri]
			obstab[i].id   = row.id
			obstab[i].ndet = len(row.detids)
			for field in simple_fields:
				obstab[i][field] = row[field]
			# Get the band sensitvity
			inds  = utils.find(detids, row.detids)
			dsens = self.dev.get(row.dsens)
			for bi, band in enumerate(ubands):
				good = bands[inds]==band
				band_dsens = dsens[good]
				if band_dsens.size == 0: continue
				band_sens  = np.sum(band_dsens**-2)**-0.5
				obstab[i].sens[bi] = band_sens
		# to degrees
		for field in deg_fields:
			obstab[field] /= utils.degree
		# Build obs,dsens matrix. A problem with this matrix is that it is
		# quite sparse for multi-tube obsjoint or multi-wafer waferjoint, and can get
		# pretty big. It would be more useful to separate out the dense blocks, but
		# not easy to do this automatically. Basically a boolean svd.
		# Will leave it as is for now
		dsens = np.zeros((nobs,ndet), np.float32)
		for i, ri in enumerate(order_obs):
			row  = self.info[ri]
			inds = utils.find(detids, row.detids)
			dsens[i,inds] = self.dev.get(row.dsens)
		res = bunch.Bunch(obstab=obstab, detids=detids, bands=bands, ubands=ubands, dsens=dsens)
		bunch.write(prefix + "info.hdf", res)
	valid_outputs = ["info"]

# Mapmaking function
def make_map(mapmaker, loader, obsinfo, comm, joint=None, inds=None, prefix=None, dump=[], maxiter=500, maxerr=1e-7, prealloc=True, ignore="recover", cont=False):
	if prefix is None: prefix = ""
	# Skip if we're already done
	if cont and (os.path.exists(prefix + ".empty") or all([signal.written(prefix) for signal in mapmaker.signals])):
		L.print("Skipped %s: Already done" % prefix, level=2, color=colors.gray)
		return
	# Groups is a list of obsinfo entries that should be mapped jointly
	# (as a big super-tod with full noise correlations)
	if joint is None: joint = trivial_joint(obsinfo)
	# Inds is a list of indices into joint groups, giving which groups this mpi task
	# should care about
	if inds is None:
		# Use the first entry in groups as representative
		gfirst = np.array([g[0] for g in joint.groups])
		dist = tiling.distribute_tods_semibrute(obsinfo[gfirst], comm.size)
		inds = np.where(dist.owner == comm.rank)[0]
	# Set up exception types we will ignore
	if   ignore == "all":     etypes, load_catch = (Exception,), "all"
	elif ignore == "missing": etypes, load_catch = (utils.DataMissing,), "expected"
	elif ignore == "recover": etypes, load_catch = (utils.DataMissing, gutils.RecoverableError), "all"
	elif ignore == "none":    etypes, load_catch = (), "none"
	else: raise ValueError("Unrecognized error ignore setting '%s'" % str(ignore))
	# Accept either a list of integers or a single integer
	try: dump = list(dump)
	except TypeError: dump = [dump]
	dev = mapmaker.dev
	# Set up memory pools. Setting these up before-hand is
	# actually more memory-efficient, as long as our estimate is
	# good.
	if prealloc and len(inds) > 0:
		ntot_max = np.max(gutils.obs_group_size(obsinfo, joint.groups, inds=inds, sampranges=joint.sampranges))
		setup_buffers(dev, ntot_max)
	# Start map from scartch
	mapmaker.reset()
	# Add our observations
	for i, ind in enumerate(inds):
		name   = joint.names[ind]
		subids = obsinfo.id[joint.groups[ind]]
		t1     = time.time()
		try:
			data = loader.load_multi(subids, samprange=joint.sampranges[ind], catch=load_catch)
		except etypes as e:
			L.print("Skipped %s: %s" % (name, str(e)), level=2, color=colors.red)
			continue
		if len(data.errors) > 0:
			# Partial skip
			L.print("Skipped parts %s" % str(data.errors[-1]), level=2, color=colors.red)
		t2    = time.time()
		try:
			mapmaker.add_obs(name, data, deslope=False)
		except etypes as e:
			L.print("Skipped %s: %s" % (name, str(e)), level=2, color=colors.red)
			continue
		dev.garbage_collect()
		del data
		t3    = time.time()
		L.print("Processed %s in %6.3f. Read %6.3f Add %6.3f" % (name, t3-t1, t2-t1, t3-t2), level=2)

	nobs = comm.allreduce(len(mapmaker.data))
	if nobs == 0:
		L.print("No tods survived!", id=0, level=0, color=colors.red)
		# Mark as done by making an .empty-file. This is used by cont
		# Annoyingly this file will be hidden for the no-prefix case,
		# since it starts with a .
		if comm.rank == 0: open(prefix + ".empty", "a").close()
		return None

	mapmaker.prepare()
	# Write rhs and ivar
	for signal in mapmaker.signals:
		signal.write_misc(prefix)

	# Solve the equation system
	for step in mapmaker.solve(maxiter=maxiter, maxerr=maxerr):
		# Dump if we're in dump-list, or a multiple of the last entry, but not if
		# we're on the final iteration anyway, since that would be redundant with
		# the final result
		will_dump = len(dump) > 0 and (step.i in dump or (dump[-1] != 0 and step.i % dump[-1] == 0)) and step.i != maxiter
		L.print("CG %4d %15.7e  %6.3f s%s" % (step.i, step.err, step.t, " (write)" if will_dump else ""), id=0, level=1, color=colors.lgreen)
		if will_dump:
			for signal, val in zip(mapmaker.signals, step.x):
				signal.write(prefix, val, suffix="%04d" % step.i)
	# Write the final result
	for signal, val in zip(mapmaker.signals, step.x):
		signal.write(prefix, val)

def make_maps_perobs(mapmaker, loader, obsinfo, comm, comm_per, joint=None, inds=None, prefix=None, dump=[], maxiter=500, maxerr=1e-7, prealloc=True, ignore="recover", cont=False):
	"""Like make_map, but makes one map per subobs. NB! The communicators in the mapmaker
	signals must be COMM_SELF for this to work. TODO: Make this simpler."""
	if joint is None:
		joint = trivial_joint(obsinfo)
	if inds is None:
		inds = list(range(comm.rank, len(joint.groups), comm.size))
	if prefix is None:
		prefix = ""
	if prealloc:
		ntot_max = np.max(gutils.obs_group_size(obsinfo, joint.groups, inds=inds, sampranges=joint.sampranges))
		setup_buffers(mapmaker.dev, ntot_max)

	# Map indivdual tods
	for ind in inds:
		name    = joint.names[ind]
		subpre  = prefix + name.replace(":","_") + "_"
		L.print("Mapping %s" % name)
		make_map(mapmaker, loader, obsinfo, comm_per, prefix=subpre, dump=dump, joint=joint, inds=[ind], maxiter=maxiter, maxerr=maxerr, prealloc=False, ignore=ignore, cont=cont)

def make_maps_depth1(mapmaker, loader, obsinfo, comm, comm_per, joint=None, prefix=None, dump=[], maxiter=500, maxerr=1e-7, fullinfo=None, prealloc=True, ignore="recover", cont=False):
	# Find scanning periods. We want to base this on the full
	# set of observations, so depth-1 maps cover consistent periods
	# even if 
	from pixell import bench
	if prefix   is None: prefix = ""
	if joint    is None: joint = trivial_joint(obsinfo)
	# FIXME: might want obs,+bad here, to make the depth-1 periods independent
	# of our cuts. But currently that causes a segfault in sqlite?
	if fullinfo is None: fullinfo = loader.query("obs")
	periods = gutils.find_scan_periods(fullinfo)
	# Which period each group belongs to. So pinds is [ngroup]
	gfirst  = np.array([g[0] for g in joint.groups])
	gpids   = utils.find_range(periods, obsinfo.ctime[gfirst]+obsinfo.dur[gfirst]/2)
	# Get rid of groups that don't belong to a period. This shouldn't happen
	bad = gpids<0
	if np.any(bad):
		L.print("Warning: %d obs with no period! %s" % (np.sum(bad), ", ".join(obsinfo.id[gfirst[bad]])), color=colors.red, id=0)
	# Indices of the groups we will map
	inds  = np.where(~bad)[0]
	# Which period each of those groups belongs to
	gpids = gpids[inds]
	# Group the groups by period. group-group i will consist
	# of groups inds[order[edges[i]:edges[i+1]]]
	pids, order, edges = utils.find_equal_groups_fast(gpids)
	my_pinds = list(range(comm.rank, len(pids), comm.size))
	if prealloc:
		# Estimate the max memory a group-group needs, and preallocate
		ntot_max  = 0
		for pind in my_pinds:
			my_inds = inds[order[edges[pind]:edges[pind+1]]]
			ntot    = np.max(gutils.obs_group_size(obsinfo, joint.groups, inds=my_inds, sampranges=joint.sampranges))
			ntot_max= max(ntot, ntot_max)
		setup_buffers(mapmaker.dev, ntot_max)
	# Now loop over and map each of our group-groups
	for pind in my_pinds:
		pid     = pids[pind]
		my_inds = inds[order[edges[pind]:edges[pind+1]]]
		# Name after period
		name    = "%10.0f" % periods[pid][0]
		subpre  = prefix + name + "_"
		L.print("Mapping period %10.0f:%10.0f with %d obs" % (*periods[pid], len(my_inds)))
		make_map(mapmaker, loader, obsinfo, comm_per, joint=joint, inds=my_inds, prefix=subpre, dump=dump, maxiter=maxiter, maxerr=maxerr, prealloc=False, ignore=ignore, cont=cont)

# Mapmaking function
def dump_tod(loader, obsinfo, noise_model, comm, joint=None, inds=None, prefix=None,
		tdown=500, fdown=250, fdown_lowf=10, fmax_lowf=4, prealloc=True, dev=None, ignore="recover"):
	if prefix is None: prefix = ""
	# Groups is a list of obsinfo entries that should be mapped jointly
	# (as a big super-tod with full noise correlations)
	if joint is None: joint = trivial_joint(obsinfo)
	# Inds is a list of indices into joint groups, giving which groups this mpi task
	# should care about
	if inds is None:
		# Use the first entry in groups as representative
		gfirst = np.array([g[0] for g in joint.groups])
		dist = tiling.distribute_tods_semibrute(obsinfo[gfirst], comm.size)
		inds = np.where(dist.owner == comm.rank)[0]
	# Set up exception types we will ignore
	if   ignore == "all":     etypes = (Exception,)
	elif ignore == "missing": etypes = (utils.DataMissing,)
	elif ignore == "recover": etypes = (utils.DataMissing, gutils.RecoverableError)
	elif ignore == "none":    etypes = ()
	else: raise ValueError("Unrecognized error ignore setting '%s'" % str(ignore))
	if dev is None: dev = device.get_device()
	# Set up memory pools. Setting these up before-hand is
	# actually more memory-efficient, as long as our estimate is
	# good.
	if prealloc and len(inds) > 0:
		ntot_max = np.max(gutils.obs_group_size(obsinfo, joint.groups, inds=inds, sampranges=joint.sampranges))
		setup_buffers(dev, ntot_max)
	# Process our observations
	for i, ind in enumerate(inds):
		name   = joint.names[ind]
		subids = obsinfo.id[joint.groups[ind]]
		subpre = prefix + name.replace(":","_") + "_"
		t1     = time.time()
		try:
			data = loader.load_multi(subids, samprange=joint.sampranges[ind])
		except etypes as e:
			L.print("Skipped %s: %s" % (name, str(e)), level=2, color=colors.red)
			continue
		if len(data.errors) > 0:
			# Partial skip
			L.print("Skipped parts %s" % str(data.errors[-1]), level=2, color=colors.red)
		t2    = time.time()

		# Output tod
		otod = dev.get(gutils.downgrade(data.tod, tdown))
		dt   = (data.ctime[-1]-data.ctime[0])/(len(data.ctime)-1)
		twcs = wcsutils.explicit(crval=[0,0], crpix=[1,1], cdelt=[dt*tdown,1])
		enmap.write_map(subpre + "tod.fits", enmap.enmap(otod, twcs))

		# Output ps
		ft   = dev.lib.rfft(data.tod)
		nsamp= data.tod.shape[1]
		normexp = -1
		ft  *= nsamp**normexp

		ps   = dev.np.abs(ft)**2
		ops  = dev.get(gutils.downgrade(ps, fdown)/data.tod.shape[1])
		df   = 1/(dt*data.tod.shape[1])
		fwcs = wcsutils.explicit(crval=[0,0], crpix=[1,1], cdelt=[df*fdown,1])
		enmap.write_map(subpre + "ps.fits", enmap.enmap(ops, fwcs))

		# Output high-res ps for low freq
		ops  = dev.get(gutils.downgrade(ps, fdown_lowf)/data.tod.shape[1])
		nbin = utils.floor(fmax_lowf/(df*fdown_lowf))
		ops  = ops[:,:nbin]
		fwcs = wcsutils.explicit(crval=[0,0], crpix=[1,1], cdelt=[df*fdown_lowf,1])
		enmap.write_map(subpre + "ps_lowf.fits", enmap.enmap(ops, fwcs))
		del ps

		# Output freq corrmat. Uses same resolution as coarse ps
		ndet, nfreq = ft.shape
		nbin = utils.floor(fmax_lowf/(df*fdown))
		bft  = ft[:,:nbin*fdown].reshape(ndet,nbin,fdown)
		cbft = dev.np.conj(bft)
		fcov = dev.np.einsum("dbf,ebf->deb",cbft,bft).real
		v    = dev.np.einsum("ddb->db", fcov)**-0.5
		fcorr= fcov*v[:,None,:]*v[None,:,:]
		fwcs = wcsutils.explicit(crval=[0,0], crpix=[1,1], cdelt=[df*fdown,1])
		enmap.write_map(subpre + "corr_lowf.fits", enmap.enmap(fcorr, fwcs))
		del bft, cbft

		# Same, but high resolution, for a subset of detectors
		ndet, nfreq = ft.shape
		thin = 100
		nbin = utils.floor(fmax_lowf/(df*fdown_lowf))
		bft  = ft[:,:nbin*fdown_lowf].reshape(ndet,nbin,fdown_lowf)
		cbft = dev.np.conj(bft[::thin])
		fcov = dev.np.einsum("dbf,ebf->deb",cbft,bft).real
		v    = dev.np.sum(dev.np.abs(bft)**2,-1)**-0.5
		fcorr= fcov*v[::thin,None,:]*v[None,:,:]
		fwcs = wcsutils.explicit(crval=[0,0], crpix=[1,1], cdelt=[df*fdown_lowf,1])
		enmap.write_map(subpre + "corr_lowf2.fits", enmap.enmap(fcorr, fwcs))
		del bft, cbft

		## Build the noise model
		#ft2  = ft.copy()
		#iN   = noise_model.build_fourier(ft2, srate=1/dt, nsamp=data.tod.shape[1])
		#d2   = dev.np.arange(ndet)
		#d1   = d2[::thin]
		#finds= dev.np.arange(nbin*fdown_lowf)
		#fcov = iN.eval_cov(d1=d1, d2=d2, finds=finds)
		## Same averaging as we do with the data
		#fcov = dev.np.sum(fcov.reshape(fcov.shape[:2]+(nbin,fdown_lowf)),-1)
		## Need the diagonal for normalization
		#v    = iN.eval_var(d=d2, finds=finds)
		#v    = dev.np.sum(v.reshape(v.shape[:1]+(nbin,fdown_lowf)),-1)**-0.5
		#fcorr= fcov*v[::thin,None,:]*v[None,:,:]
		#fwcs = wcsutils.explicit(crval=[0,0], crpix=[1,1], cdelt=[df*fdown_lowf,1])
		#enmap.write_map(subpre + "corrmodel_lowf2.fits", enmap.enmap(fcorr, fwcs))

		## Choose a single nmat bin and see what happens there.
		## Let's try a bin that's wider than the number of detectors to be safe
		#bi = np.where(iN.bins[:,1]-iN.bins[:,0] > ndet//8)[0][0]
		#b1, b2 = iN.bins[bi]
		#debug = bunch.Bunch(bi=bi, bin=iN.bins[bi], freqs=iN.bins[bi]*df,
		#	V=iN.V[bi], iD=iN.iD[bi], iE=iN.iE[bi], dpre=ft2[:,b1:b2].copy())

		## Try to manually project out the dark modes
		#dark  = np.array(["DARK" in band for band in data.bands])
		#light = ~dark
		#iiN11 = iN.det_slice(light).inv()
		#d2    = ft2.copy(); d2[light] = 0
		#ft2[light] += iiN11.apply(iN.apply(d2, nofft=True)[light], nofft=True)

		#debug.dark  = dark
		#debug.dpost = ft2[:,b1:b2]
		#bunch.write(subpre + "debug.hdf", debug)

		## Output high-res ps for low freq
		#ps   = dev.np.abs(ft)**2
		#ops  = dev.get(gutils.downgrade(ps, fdown_lowf)/data.tod.shape[1])
		#nbin = utils.floor(fmax_lowf/(df*fdown_lowf))
		#ops  = ops[:,:nbin]
		#fwcs = wcsutils.explicit(crval=[0,0], crpix=[1,1], cdelt=[df*fdown_lowf,1])
		#enmap.write_map(subpre + "ps_deproj.fits", enmap.enmap(ops, fwcs))
		#del ps

		#nbin = utils.floor(fmax_lowf/(df*fdown_lowf))
		#bft  = ft[:,:nbin*fdown_lowf].reshape(ndet,nbin,fdown_lowf)
		#cbft = dev.np.conj(bft[::thin])
		#fcov = dev.np.einsum("dbf,ebf->deb",cbft,bft).real
		#v    = dev.np.sum(dev.np.abs(bft)**2,-1)**-0.5
		#fcorr= fcov*v[::thin,None,:]*v[None,:,:]
		#fwcs = wcsutils.explicit(crval=[0,0], crpix=[1,1], cdelt=[df*fdown_lowf,1])
		#enmap.write_map(subpre + "corr_deproj.fits", enmap.enmap(fcorr, fwcs))

		dev.garbage_collect()
		del data
		t3    = time.time()
		L.print("Processed %s in %6.3f. Read %6.3f Add %6.3f" % (name, t3-t1, t2-t1, t3-t2), level=2)

def setup_buffers(dev, ntot, dtype=np.float32, ndet_guess=1000):
	"""Pre-allocate memory buffers for mapmaking. Pass in the worst-case
	ntot = ndet*nsamp.

	Pre-allocation isn't really necessary,
	but automatically growing the buffers has some overhead, and I've unexpectedly
	run out of memory when not doing this. This should be investigated futher.

	This function will need to be updated if which memory pools are used is changed."""
	# These are the big ones
	ctype = utils.complex_dtype(dtype)
	ftot = ntot//2 + ndet_guess
	dev.pools["pointing"]   .empty((3, ntot), dtype=dtype)
	dev.pools["tod"]        .empty(ntot, dtype=dtype)
	dev.pools["ft" ]        .empty(ftot, dtype=ctype)
	dev.pools["fft_scratch"].empty(ftot, dtype=ctype)
	dev.pools.reset()
	# The remaining are:
	# * plan
	# * cut
	# * sky_iwork
	# * sky_owork
	# * nmat_work
	# We don't have good estimates for these, so we don't pre-allocate them.
	# But they should be small

def trivial_joint(obsids):
	"""Make a group-info corresponding to a no grouping"""
	groups=[[i] for i in range(len(obsids))]
	return bunch.Bunch(groups=groups, names=obsids.id, sampranges=[None for i in range(len(obsids))])

# Zippers
# These package our degrees of freedomf or the CG solver

class ArrayZipper:
	def __init__(self, shape, dtype, comm=None):
		self.shape = shape
		self.ndof  = int(np.prod(shape))
		self.dtype = dtype
		self.comm  = comm
	def zip(self, arr):  return arr.reshape(-1)
	def unzip(self, x):  return x.reshape(self.shape).astype(self.dtype, copy=False)
	def dot(self, a, b):
		return np.sum(a*b) if self.comm is None else self.comm.allreduce(np.sum(a*b))

class MultiZipper:
	def __init__(self):
		self.zippers = []
		self.ndof    = 0
		self.bins    = []
	def add(self, zipper):
		self.zippers.append(zipper)
		self.bins.append([self.ndof, self.ndof+zipper.ndof])
		self.ndof += zipper.ndof
	def zip(self, objs):
		return np.concatenate([zipper.zip(obj) for zipper, obj in zip(self.zippers, objs)])
	def unzip(self, x):
		res = []
		for zipper, (b1,b2) in zip(self.zippers, self.bins):
			res.append(zipper.unzip(x[b1:b2]))
		return res
	def dot(self, a, b):
		res = 0
		for (b1,b2), dof in zip(self.bins, self.zippers):
			res += dof.dot(a[b1:b2],b[b1:b2])
		return res

def extend_prefix(prefix, extra):
	if len(prefix) == 0 or prefix.endswith("/"):
		return prefix + extra
	else:
		return prefix + "_" + extra

def check_outputs(outputs, valid_outputs, name):
	if   outputs is None: return []
	elif outputs == "all": return valid_outputs
	else:
		res = []
		for output in outputs:
			if output not in valid_outputs:
				raise ValueError("'%s' is not a valid output for %s" % (str(output), name))
			res.append(output)
		return res
